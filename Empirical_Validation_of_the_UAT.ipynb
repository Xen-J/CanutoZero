{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xen-J/CanutoZero/blob/main/Empirical_Validation_of_the_UAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ctL7WuAlG8Z"
      },
      "source": [
        "#About the experiment\n",
        "Shallow neural network with varying number of neurons, activation functions (tanh, relu and sigmoid) and target functions (sin, gaussian, sin_high_freq).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting the data in a full-results .csv file\n"
      ],
      "metadata": {
        "id": "cpRE0QTtQXuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_results(all_results, filename=\"full_results.csv\"):\n",
        "    \"\"\"\n",
        "    It creates a CSV where each row has the following structure:\n",
        "    target_function, act_function, n_neurons, ...\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # 1) Iterate through the all_results.items\n",
        "    for (func, act), (results_dict, metrics) in all_results.items():\n",
        "        for n, res in results_dict.items():\n",
        "\n",
        "            row = {\n",
        "                \"target_function\": func,\n",
        "                \"act_function\": act,\n",
        "                \"n_neurons\": n,\n",
        "            }\n",
        "\n",
        "            for key, value in res.items():\n",
        "                if key!=\"losses\" and key!=\"final_pred\":\n",
        "                    row[key] = value\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "    # 2) Columns\n",
        "    all_keys = set()\n",
        "    for r in rows:\n",
        "        all_keys.update(r.keys())\n",
        "    all_keys = list(all_keys)\n",
        "\n",
        "    # 3) Write CSV\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    print(f\"Saved: {filename}\")\n"
      ],
      "metadata": {
        "id": "WaIwt-KgQf-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yj8e513lSyf"
      },
      "source": [
        "##Core module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVp5B-D-lAM9",
        "outputId": "458fea1a-483c-497a-b53a-dd7bb158e3b6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + tanh\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 1000\n",
            "  Testing: [5, 10]\n",
            "  Epochs: 3000, LR: 0.8\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 1000, Test Samples: 1000\n",
            "\n",
            "  Training 5 neurons...\n",
            "      Epoch 1000/3000: Loss = 0.408293\n",
            "      Epoch 2000/3000: Loss = 0.407103\n",
            "      Epoch 3000/3000: Loss = 0.406879\n",
            "    ✗ MSE = 0.394466, Time = 1.8s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/3000: Loss = 0.010064\n",
            "      Epoch 2000/3000: Loss = 0.123600\n",
            "      Epoch 3000/3000: Loss = 0.070045\n",
            "    ✗ MSE = 0.087839, Time = 1.8s\n",
            "   Saved: results_try/sin/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = None, Best MSE = 0.087839\n",
            "\n",
            "N_min: None\n",
            "Best MSE: 0.087839 at N=10\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Core Module for UAT Validation Experiments\n",
        "===========================================\n",
        "Code for validating the Universal Approximation Theorem empirically.\n",
        "It defines the experiment configuration, neural network architecture,\n",
        "data generation, training routines, metrics computation, and visualization.\n",
        "To run RunAll.py, import this module.\n",
        "\n",
        "Authors: Xendra Jaime Reyes, Queen-Aset Blissett\n",
        "Date: November 2025\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION: ExperimentConfig will define the experiment parameters,\n",
        "# such as neuron counts, activation functions, target functions, training epochs,\n",
        "# learning rates, domain boundaries, convergence thresholds and others.\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Configuration for UAT validation experiments.\n",
        "\n",
        "    Attributes:\n",
        "        neuron_counts: List of hidden layer widths to test\n",
        "        activation: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "        target_function: Function to approximate ('sin', 'sin_high_freq', 'gaussian')\n",
        "        epochs: Maximum training epochs (we'll use 1000-epoch batches)\n",
        "        learning_rate: Adam optimizer learning rate\n",
        "        x_min, x_max: Domain boundaries\n",
        "        n_samples: Number of training points\n",
        "        threshold: MSE convergence threshold\n",
        "        random_seed: For reproducibility\n",
        "        save_dir: Directory to save results\n",
        "        verbose: Print training progress\n",
        "    \"\"\"\n",
        "\n",
        "    #-----We define default values-----#\n",
        "    # Network architecture\n",
        "    neuron_counts: List[int] = field(default_factory=lambda: [10, 50, 100, 500, 1000])\n",
        "    activation: str = 'relu'\n",
        "\n",
        "    target_function: str = 'sin'    #target function to approximate\n",
        "    threshold: float = 0.01         # MSE convergence threshold\n",
        "    random_seed: int = 42           # Random seed for reproducibility\n",
        "\n",
        "    # Training hyperparameters\n",
        "    epochs: int = 5000\n",
        "    learning_rate: float = 0.01\n",
        "\n",
        "    # Domain\n",
        "    x_min: float = -3.0\n",
        "    x_max: float = 3.0\n",
        "    n_samples: int = 1000\n",
        "\n",
        "    # Output\n",
        "    save_dir: str = 'results'\n",
        "    verbose: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration.\"\"\"\n",
        "        valid_activations = ['relu', 'tanh', 'sigmoid']\n",
        "        if self.activation not in valid_activations:\n",
        "            raise ValueError(f\"activation must be one of {valid_activations}\")\n",
        "\n",
        "        valid_functions = ['sin', 'sin_high_freq', 'gaussian', 'smooth_stair']\n",
        "        if self.target_function not in valid_functions:\n",
        "            raise ValueError(f\"target_function must be one of {valid_functions}\")\n",
        "\n",
        "# ============================================================================\n",
        "# NEURAL NETWORK: To prove the UAT, we will use only a Single Hidden Layer\n",
        "# architecture (SingleHiddenLayerNet) with personalised number of neurons\n",
        "# and activation function.\n",
        "# ============================================================================\n",
        "\n",
        "class SingleHiddenLayerNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Single hidden layer neural network for function approximation.\n",
        "\n",
        "    Architecture:\n",
        "        Input (1D) → Hidden (n_neurons) → Activation → Output (1D)\n",
        "\n",
        "    Args:\n",
        "        n_neurons: Number of hidden layer neurons\n",
        "        activation: Activation function name\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neurons: int, activation: str = 'relu'):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(1, n_neurons)\n",
        "        self.output = nn.Linear(n_neurons, 1)\n",
        "        self.activation_name = activation\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 1)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        h = self.hidden(x)\n",
        "\n",
        "        # Apply activation, we are only working with 3 types\n",
        "        if self.activation_name == 'relu':\n",
        "            h = torch.relu(h)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            h = torch.tanh(h)\n",
        "        elif self.activation_name == 'sigmoid':\n",
        "            h = torch.sigmoid(h)\n",
        "\n",
        "        y = self.output(h)\n",
        "        return y\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\"Count total trainable parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA GENERATION: Depending on the target function, we will generate training\n",
        "# data and a suitable label for visualization.\n",
        "# ============================================================================\n",
        "\n",
        "def generate_target_data(config: ExperimentConfig) -> Tuple[torch.Tensor, torch.Tensor, str]:\n",
        "    \"\"\"\n",
        "    Generate training data and the full dense domain set for plotting/testing.\n",
        "    Args:\n",
        "        config: Experiment configuration\n",
        "\n",
        "    Returns:\n",
        "        x_train: Input tensor of shape (n_samples, 1)\n",
        "        y_train: Target output tensor of shape (n_samples, 1)\n",
        "        x_test: Test input tensor of shape (n_samples, 1)\n",
        "        y_test: Test output tensor of shape (n_samples, 1)\n",
        "        label: Human-readable function label\n",
        "    \"\"\"\n",
        "    #We generate a dense set\n",
        "    x_dense = torch.linspace(config.x_min, config.x_max, config.n_samples * 2).reshape(-1, 1)\n",
        "\n",
        "    if config.target_function == 'sin':\n",
        "        y_dense = torch.sin(np.pi * x_dense)\n",
        "        label = 'sin(x)'\n",
        "    elif config.target_function == 'sin_high_freq':\n",
        "        y_dense = torch.sin(2 * np.pi * x_dense)\n",
        "        label = 'sin(2πx)'\n",
        "    elif config.target_function == 'gaussian':\n",
        "        y_dense = torch.exp(-x_dense**2)\n",
        "        label = 'exp(-x²)'\n",
        "    elif config.target_function == 'composite':\n",
        "        y_dense = torch.sin(2 * np.pi * x_dense) * torch.exp(-0.3 * x_dense**2)\n",
        "        label = 'sin(2πx)·exp(-0.3x²)'\n",
        "\n",
        "    #Random index to ensure train/test split\n",
        "    idxs = torch.randperm(len(x_dense))\n",
        "    n_train = config.n_samples\n",
        "\n",
        "    train_idxs = idxs[:n_train]\n",
        "\n",
        "    x_train = x_dense[train_idxs]\n",
        "    y_train = y_dense[train_idxs]\n",
        "\n",
        "    return x_train, y_train, x_dense, y_dense, label\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING: According to the neural network built architecture, we will train\n",
        "# the model using Adam optimizer, returning a dictionary with training results,\n",
        "# such as losses, final predictions, MSE, MAE, max error, convergence epoch,\n",
        "# training time, and convergence status.\n",
        "# ============================================================================\n",
        "\n",
        "def train_network(\n",
        "    model: nn.Module,\n",
        "    x_train: torch.Tensor,\n",
        "    y_train: torch.Tensor,\n",
        "    config: ExperimentConfig\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Train neural network to approximate target function.\n",
        "\n",
        "    Args:\n",
        "        model: Neural network to train\n",
        "        x_train: Training inputs\n",
        "        y_train: Training targets\n",
        "        config: Training configuration\n",
        "\n",
        "    Returns:\n",
        "        Tuple[model, Dict]: The trained model and dictionary of partial results.\n",
        "        Dict dictionary contains:\n",
        "            - losses: List of training losses per epoch\n",
        "            - final_pred: Final predictions on training set\n",
        "            - final_mse: Final mean squared error\n",
        "            - final_mae: Final mean absolute error\n",
        "            - max_error: Maximum pointwise error\n",
        "            - convergence_time: seconds since start time when MSE < threshold (or None)\n",
        "            - training_time: Wall-clock training time in seconds\n",
        "            - converged: Boolean indicating convergence\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    losses = []\n",
        "    convergence_time = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        # Forward pass\n",
        "        y_pred = model(x_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item()) # Record loss\n",
        "\n",
        "        # Check convergence\n",
        "        if loss.item() < config.threshold and convergence_time ==0:\n",
        "            convergence_time = time.time() - start_time\n",
        "\n",
        "        # Verbose output\n",
        "        if config.verbose and (epoch + 1) % 1000 == 0:\n",
        "            print(f\"      Epoch {epoch+1}/{config.epochs}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Final evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        final_pred = model(x_train)\n",
        "        final_mse_train = criterion(final_pred, y_train).item()\n",
        "\n",
        "    return model, {\n",
        "        'losses': losses,\n",
        "        'convergence_time': convergence_time if convergence_time else -1,\n",
        "        'training_time': training_time,\n",
        "        'converged': final_mse_train < config.threshold,\n",
        "  }\n",
        "\n",
        "# ============================================================================\n",
        "# METRICS: Taking the experimental results, compute_metrics will return\n",
        "# a metric summary dictionary, including minimum viable width (N_min),\n",
        "# marginal efficiency, best MSE and corresponding network width.\n",
        "# This way, we can compare Neuron efficiency, Convergence speed, and\n",
        "# Approximation accuracy per activation function.\n",
        "# ============================================================================\n",
        "\n",
        "def compute_metrics(results: Dict, neuron_counts: List[int]) -> Dict:\n",
        "    \"\"\"\n",
        "    Compute summary metrics from experimental results.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary mapping neuron_count → training results\n",
        "        neuron_counts: List of tested network widths\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing:\n",
        "            - n_min: Minimum viable width (or None if never converges)\n",
        "            - marginal_efficiency: List of MSE improvement per neuron added\n",
        "            - best_mse: Lowest achieved MSE\n",
        "            - best_n: Network width achieving best MSE\n",
        "            - fastest_n: Width with shortest training time.\n",
        "            - fastest_time: Fastest training time.\n",
        "            - quickest_conv_n: Network width with shortest convergence epoch.\n",
        "            - quickest_conv_epoch: Shortest convergence epoch.\n",
        "    \"\"\"\n",
        "    #--- Minimum viable width (N_min)---#\n",
        "    n_min = next((n for n in neuron_counts if results[n]['converged']), None)\n",
        "\n",
        "    #--- Marginal efficiency---#\n",
        "    mse_values = [results[n]['final_mse'] for n in neuron_counts]\n",
        "    marginal_eff = []\n",
        "    for i in range(1, len(neuron_counts)):\n",
        "        delta_n = neuron_counts[i] - neuron_counts[i-1]\n",
        "        delta_mse = mse_values[i-1] - mse_values[i]\n",
        "        eff = delta_mse / delta_n if delta_n > 0 else 0.0\n",
        "        marginal_eff.append(eff)\n",
        "\n",
        "    # ---Best performance (MSE and R^2) ---#\n",
        "    best_idx = np.argmin(mse_values)\n",
        "    best_mse = mse_values[best_idx]\n",
        "    best_n = neuron_counts[best_idx]\n",
        "    best_r2 = results[best_n]['final_r2'] # We use N of best MSE\n",
        "\n",
        "    #--- Velocity (time and epochs) ---#\n",
        "    times = [results[n]['training_time'] for n in neuron_counts]\n",
        "    fastest_idx = np.argmin(times)\n",
        "    fastest_time = times[fastest_idx]\n",
        "    fastest_n = neuron_counts[fastest_idx]\n",
        "\n",
        "    conv_epochs = [results[n]['convergence_time'] for n in neuron_counts if results[n]['converged']]\n",
        "    conv_neurons = [n for n in neuron_counts if results[n]['converged']]\n",
        "\n",
        "    quickest_conv_epoch = None\n",
        "    quickest_conv_n = None\n",
        "\n",
        "    if conv_epochs:\n",
        "        quickest_idx = np.argmin(conv_epochs)\n",
        "        quickest_conv_epoch = conv_epochs[quickest_idx]\n",
        "        quickest_conv_n = conv_neurons[quickest_idx]\n",
        "\n",
        "\n",
        "    return {\n",
        "        'n_min': n_min,\n",
        "        'marginal_efficiency': marginal_eff,\n",
        "        'best_mse': best_mse,\n",
        "        'best_r2': best_r2,\n",
        "        'best_n': best_n,\n",
        "        'fastest_time': fastest_time,\n",
        "        'fastest_n': fastest_n,\n",
        "        'quickest_conv_epoch': quickest_conv_epoch,\n",
        "        'quickest_conv_n': quickest_conv_n,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST: We evaluate the neural network on the given test set.\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_network(model: nn.Module, x_test: torch.Tensor, y_test: torch.Tensor) -> Dict:\n",
        "    \"\"\"Calculte all precistion metrics on the test set.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_pred = model(x_test)\n",
        "\n",
        "        # 1. MSE\n",
        "        final_mse = nn.MSELoss()(test_pred, y_test).item()\n",
        "\n",
        "        # 2. MAE\n",
        "        final_mae = torch.mean(torch.abs(test_pred - y_test)).item()\n",
        "\n",
        "        # 3. Max Error\n",
        "        max_error = torch.max(torch.abs(test_pred - y_test)).item()\n",
        "\n",
        "        # 4. R^2 Score\n",
        "        # RSS: Residual Sum of Squares (Error no explicado)\n",
        "        RSS = torch.sum((y_test - test_pred) ** 2)\n",
        "        # SST: Total Sum of Squares (Varianza total)\n",
        "        SST = torch.sum((y_test - torch.mean(y_test)) ** 2)\n",
        "        final_r2 = 1 - (RSS / SST).item() if SST != 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'final_pred': test_pred.numpy(), # Usamos las predicciones del test set para el gráfico\n",
        "        'final_mse': final_mse,\n",
        "        'final_mae': final_mae,\n",
        "        'max_error': max_error,\n",
        "        'final_r2': final_r2,\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION: Plot_experiment_results will generate comprehensive images\n",
        "# of the experimental results dictionary, including a convergence plot,\n",
        "# function approximation plots for every network size, error distributions\n",
        "# comparing each network size, and a summary text box per image.\n",
        "# ============================================================================\n",
        "\n",
        "def plot_experiment_results(\n",
        "    results: Dict,\n",
        "    x: torch.Tensor,\n",
        "    y_true: torch.Tensor,\n",
        "    func_label: str,\n",
        "    config: ExperimentConfig,\n",
        "    save_path: Optional[str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate comprehensive visualization of experimental results.\n",
        "\n",
        "    Creates a figure with:\n",
        "        - Convergence plot (MSE vs epoch for all network sizes)\n",
        "        - Function approximation plots (one per network size)\n",
        "        - Error distribution plot\n",
        "        - Summary text box\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary mapping neuron_count → training results\n",
        "        x: Input data\n",
        "        y_true: Target outputs\n",
        "        func_label: Human-readable function label\n",
        "        config: Experiment configuration\n",
        "        save_path: Optional path to save figure\n",
        "    \"\"\"\n",
        "    x_np = x.numpy().flatten()\n",
        "    y_true_np = y_true.numpy().flatten()\n",
        "\n",
        "    n_sizes = len(config.neuron_counts)\n",
        "\n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(18, 10))\n",
        "\n",
        "    # Color for this activation\n",
        "    color_map = {'relu': '#1f77b4', 'tanh': '#2ca02c', 'sigmoid': '#d62728'}\n",
        "    color = color_map[config.activation]\n",
        "\n",
        "    # ========================================\n",
        "    # PLOT 1 (CONVERGENCE): MSE vs Epoch for all network sizes,\n",
        "    # with threshold line.\n",
        "    # ========================================\n",
        "    ax_conv = plt.subplot(2, n_sizes + 2, 1)\n",
        "\n",
        "    for n in config.neuron_counts:\n",
        "        losses = results[n]['losses']\n",
        "        ax_conv.semilogy(losses, linewidth=2, label=f'{n}N', alpha=0.8)\n",
        "\n",
        "    ax_conv.axhline(y=config.threshold, color='red', linestyle='--',\n",
        "                   linewidth=1.5, alpha=0.7, label='Threshold')\n",
        "    ax_conv.set_xlabel('Epoch', fontsize=11)\n",
        "    ax_conv.set_ylabel('MSE (log scale)', fontsize=11)\n",
        "    ax_conv.set_title(f'{config.activation.upper()}: Convergence',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "    ax_conv.legend(fontsize=9, loc='upper right')\n",
        "    ax_conv.grid(True, alpha=0.3)\n",
        "\n",
        "    # ========================================\n",
        "    # PLOTS 2-n (APPROXIMATIONS): One per network size,\n",
        "    # plots comparing the true function and the NN prediction\n",
        "    # ========================================\n",
        "    for idx, n in enumerate(config.neuron_counts):\n",
        "        ax = plt.subplot(2, n_sizes + 2, idx + 2)\n",
        "\n",
        "        #---Getting predictions and metrics\n",
        "        y_pred = results[n]['final_pred'].flatten()\n",
        "        mse = results[n]['final_mse']\n",
        "        converged = results[n]['converged']\n",
        "\n",
        "        #---Graphing the true function and predicted function\n",
        "        ax.plot(x_np, y_true_np, 'k-', linewidth=2.5,\n",
        "               label=f'True: {func_label}', alpha=0.8) #True function\n",
        "        ax.plot(x_np, y_pred, '--', linewidth=2.5,\n",
        "               color=color, label='NN', alpha=0.8) #Predicted function\n",
        "        ax.fill_between(x_np, y_true_np, y_pred, alpha=0.25, color=color) # Error area\n",
        "\n",
        "        #---Title with status\n",
        "        status = \"✓\" if converged else \"✗\"\n",
        "        ax.set_title(f'{n}N: MSE={mse:.6f} {status}',\n",
        "                    fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('x', fontsize=10)\n",
        "        ax.set_ylabel('y', fontsize=10)\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ========================================\n",
        "    # PLOT n+1 (ERROR DISTRIBUTION): Comparing absolute error\n",
        "    # for every point in the domain, per network size.\n",
        "    # ========================================\n",
        "    ax_err = plt.subplot(2, n_sizes + 2, n_sizes + 3)\n",
        "\n",
        "    for n in config.neuron_counts:\n",
        "        y_pred = results[n]['final_pred'].flatten()\n",
        "        errors = np.abs(y_true_np - y_pred)\n",
        "        ax_err.plot(x_np, errors, linewidth=2, label=f'{n}N', alpha=0.8)\n",
        "\n",
        "    ax_err.axhline(y=config.threshold, color='red', linestyle='--',\n",
        "                  linewidth=1.5, alpha=0.7, label='Threshold')\n",
        "    ax_err.set_xlabel('x', fontsize=11)\n",
        "    ax_err.set_ylabel('|f(x) - g(x)|', fontsize=11)\n",
        "    ax_err.set_title('Error Distribution', fontsize=12, fontweight='bold')\n",
        "    ax_err.legend(fontsize=9)\n",
        "    ax_err.grid(True, alpha=0.3)\n",
        "\n",
        "    # ========================================\n",
        "    # PLOT n+2 (SUMMARY): Text box summarizing key metrics\n",
        "    # ========================================\n",
        "    ax_summary_a = plt.subplot(2, n_sizes + 2, n_sizes + 4)\n",
        "    ax_summary_a.axis('off')\n",
        "\n",
        "    summary_text_a = f\"{config.activation.upper()} - SUMMARY\\n{'='*35}\\n\\n\"\n",
        "\n",
        "    for n in config.neuron_counts:\n",
        "        res = results[n]\n",
        "        status = \"✓ CONVERGED\" if res['converged'] else \"✗ NOT CONVERGED\"\n",
        "\n",
        "        summary_text_a += f\"{n} NEURONS: {status}\\n\"\n",
        "        summary_text_a += f\"  MSE:                 {res['final_mse']:.6f}\\n\"\n",
        "        summary_text_a += f\"  MAE:                 {res['final_mae']:.6f}\\n\"\n",
        "        summary_text_a += f\"  Max Error:           {res['max_error']:.6f}\\n\"\n",
        "        summary_text_a += f\"  Training time:       {res['training_time']:.1f}s\\n\"\n",
        "        summary_text_a += f\"  Convergence time:    {res['convergence_time']}\\n\\n\"\n",
        "        summary_text_a += f\"  R^2:                 {res['final_r2']}\\n\\n\"\n",
        "\n",
        "    # Add metrics\n",
        "    ax_summary_b = plt.subplot(2, n_sizes + 2, n_sizes + 5)\n",
        "    ax_summary_b.axis('off')\n",
        "    metrics = compute_metrics(results, config.neuron_counts)\n",
        "\n",
        "    summary_text_b = f\"EXPERIMENT:\\n{'='*35}\\n\\n\"\n",
        "    summary_text_b += f\"Learning rate:  {config.learning_rate}\"\n",
        "    summary_text_b += f\"Threshold:      {config.threshold}\"\n",
        "    summary_text_b += f\"Epochs:         {config.epochs}\\n\"\n",
        "\n",
        "    summary_text_b += f\"{'='*35}METRICS:\\n{'='*35}\\n\\n\"\n",
        "    if metrics['n_min']:\n",
        "        summary_text_b += f\"N_min = {metrics['n_min']} neurons\\n\"\n",
        "    else:\n",
        "        summary_text_b += f\"N_min > {max(config.neuron_counts)}\\n\"\n",
        "\n",
        "    summary_text_b += f\"Best MSE = {metrics['best_mse']:.6f}\\n\"\n",
        "    summary_text_b += f\"  at N = {metrics['best_n']} neurons\\n\"\n",
        "    summary_text_b += f\"  R^2 = {metrics['best_r2']}\\n\\n\"\n",
        "\n",
        "    ax_summary_a.text(0.05, 0.95, summary_text_a, transform=ax_summary_a.transAxes,\n",
        "                   fontsize=9, verticalalignment='top', family='monospace',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "    ax_summary_b.text(0.05, 0.95, summary_text_b, transform=ax_summary_b.transAxes,\n",
        "                   fontsize=9, verticalalignment='top', family='monospace',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "    # Overall title\n",
        "    fig.suptitle(f'Approximating {func_label} with {config.activation.upper()} activation',\n",
        "                fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   Saved: {save_path}\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT RUNNER: run_single_experiment will execute a complete experiment\n",
        "# for a given configuration, returning the results dictionary and summary metrics.\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def run_single_experiment(config: ExperimentConfig) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"\n",
        "    Run complete experiment for given configuration.\n",
        "\n",
        "    Args:\n",
        "        config: Experiment configuration\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary mapping neuron_count → training results\n",
        "        metrics: Summary metrics dictionary\n",
        "    \"\"\"\n",
        "    # Set seed\n",
        "    torch.manual_seed(config.random_seed)\n",
        "    np.random.seed(config.random_seed)\n",
        "\n",
        "    if config.verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"EXPERIMENT: {config.target_function} + {config.activation}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    # Generate data\n",
        "    x_train, y_train, x_plot, y_plot, label = generate_target_data(config)\n",
        "\n",
        "    if config.verbose:\n",
        "        print(f\"  Function: {label}\")\n",
        "        print(f\"  Domain: [{config.x_min}, {config.x_max}]\")\n",
        "        print(f\"  Samples: {config.n_samples}\")\n",
        "        print(f\"  Testing: {config.neuron_counts}\")\n",
        "        print(f\"  Epochs: {config.epochs}, LR: {config.learning_rate}\")\n",
        "        print(f\"  Threshold: {config.threshold}\\n\")\n",
        "        # Print the size of the test sample\n",
        "        print(f\"  Train Samples: {x_train.shape[0]}, Test Samples: {x_plot.shape[0] - x_train.shape[0]}\")\n",
        "\n",
        "    # Train networks\n",
        "    results = {}\n",
        "\n",
        "    for n in config.neuron_counts:\n",
        "        if config.verbose:\n",
        "            print(f\"\\n  Training {n} neurons...\")\n",
        "\n",
        "        #---1. TRAINING: Returns the model and training results\n",
        "        trained_model, training_result = train_network(\n",
        "            SingleHiddenLayerNet(n, config.activation),\n",
        "            x_train,\n",
        "            y_train,\n",
        "            config\n",
        "        )\n",
        "        #---2. EVALUATION: Evaluate on test set\n",
        "        accuracy_results = evaluate_network(trained_model, x_plot, y_plot)\n",
        "\n",
        "        #---3. COMBINE RESULTS\n",
        "        results[n] = {**training_result, **accuracy_results}\n",
        "\n",
        "        if config.verbose:\n",
        "            status = \"✓\" if results[n]['converged'] else \"✗\"\n",
        "            #Test MSE\n",
        "            print(f\"    {status} MSE = {results[n]['final_mse']:.6f}, Time = {results[n]['training_time']:.1f}s\")\n",
        "\n",
        "    # Compute metrics (final MSE of training set)\n",
        "    metrics = compute_metrics(results, config.neuron_counts)\n",
        "\n",
        "    # Plot\n",
        "    save_dir = Path(config.save_dir) / config.target_function\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    save_path = save_dir / f\"{config.activation}.png\"\n",
        "\n",
        "    plot_experiment_results(results, x_plot, y_plot, label, config, str(save_path))\n",
        "\n",
        "    if config.verbose:\n",
        "        print(f\"\\n  EXPERIMENT COMPLETE! N_min = {metrics['n_min']}, Best MSE = {metrics['best_mse']:.6f}\")\n",
        "\n",
        "    return results, metrics\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON TABLE: print_comparison_table will generate a summary table\n",
        "# comparing all activations across target functions, showing N_min, best MSE,\n",
        "# and corresponding network width.\n",
        "# ============================================================================\n",
        "\n",
        "def print_comparison_table(all_results: Dict[str, Dict]):\n",
        "    \"\"\"\n",
        "    Print comparison table across activations.\n",
        "\n",
        "    Args:\n",
        "        all_results: Dictionary mapping (function, activation) → (results, metrics)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON TABLE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n{'Function':<15} {'Activation':<12} {'N_min':<10} {'Best MSE':<12} {'Best N':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for (func, act), (results, metrics) in all_results.items():\n",
        "        n_min = metrics['n_min'] if metrics['n_min'] else '>5000'\n",
        "        print(f\"{func:<15} {act:<12} {str(n_min):<10} {metrics['best_mse']:<12.6f} {metrics['best_n']:<10}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    config = ExperimentConfig(\n",
        "        neuron_counts=[5,10],\n",
        "        activation='tanh',\n",
        "        learning_rate=0.8,\n",
        "        target_function='sin',\n",
        "        verbose=True,\n",
        "        save_dir='results_try',\n",
        "        epochs=3000\n",
        "    )\n",
        "\n",
        "    results, metrics = run_single_experiment(config)\n",
        "    print(f\"\\nN_min: {metrics['n_min']}\")\n",
        "    print(f\"Best MSE: {metrics['best_mse']:.6f} at N={metrics['best_n']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQFlsBnslqCP"
      },
      "source": [
        "##Run All code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete experiment"
      ],
      "metadata": {
        "id": "BAl--p_o-fnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run All Experiments\n",
        "===================\n",
        "Execute complete experimental suite for UAT validation.\n",
        "\n",
        "Usage:\n",
        "    python run_experiments.py\n",
        "\n",
        "This will generate all plots and save to results/ directory.\n",
        "\"\"\"\n",
        "\n",
        "#In case we are local, we need to import the previous code.\n",
        "#from CoreComparison import ExperimentConfig, run_single_experiment, print_comparison_table\n",
        "import itertools\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run all experiments.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"UNIVERSAL APPROXIMATION THEOREM: EXPERIMENTAL VALIDATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\\nThis will run MxN experiments\n",
        "      (M = number of target_functions and N= number of activations)\"\"\")\n",
        "\n",
        "    # Define experimental grid\n",
        "    target_functions = ['sin', 'gaussian', 'sin_high_freq']\n",
        "    activations = ['relu', 'tanh', 'sigmoid']\n",
        "    print(\"In our case, that is \",\n",
        "          len(target_functions)*len(activations),\" experiments\")\n",
        "\n",
        "    # Shared configuration\n",
        "    base_config = {\n",
        "        'neuron_counts': [1,10,50,100,500,1000],\n",
        "        'epochs': 5000,\n",
        "        'threshold': 0.01,\n",
        "        'learning_rate': 0.01,\n",
        "        'n_samples': 3000,\n",
        "        'save_dir': 'results/complete',\n",
        "        'verbose': True,\n",
        "        'random_seed': 42,\n",
        "    }\n",
        "\n",
        "    # Run all combinations\n",
        "    all_results = {}\n",
        "\n",
        "    for func, act in itertools.product(target_functions, activations):\n",
        "        config = ExperimentConfig(\n",
        "            target_function=func,\n",
        "            activation=act,\n",
        "            **base_config\n",
        "        )\n",
        "\n",
        "        results, metrics = run_single_experiment(config)\n",
        "        all_results[(func, act)] = (results, metrics)\n",
        "\n",
        "    # Export experiment\n",
        "    save_results(all_results)\n",
        "    # Print comparison\n",
        "    print_comparison_table(all_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL EXPERIMENTS COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nResults saved in results/ directory:\")\n",
        "    print(\"  - results/sin/{relu,tanh,sigmoid}.png\")\n",
        "    print(\"  - results/sin_high_freq/{relu,tanh,sigmoid}.png\")\n",
        "    print(\"  - results/gaussian/{relu,tanh,sigmoid}.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TB6BiZHz-h3X",
        "outputId": "cd5f3d28-1699-42f3-a103-ddcd7a963825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "UNIVERSAL APPROXIMATION THEOREM: EXPERIMENTAL VALIDATION\n",
            "================================================================================\n",
            "\n",
            "This will run MxN experiments\n",
            "      (M = number of target_functions and N= number of activations)\n",
            "In our case, that is  9  experiments\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + relu\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.465637\n",
            "      Epoch 2000/5000: Loss = 0.465637\n",
            "      Epoch 3000/5000: Loss = 0.465637\n",
            "      Epoch 4000/5000: Loss = 0.465637\n",
            "      Epoch 5000/5000: Loss = 0.465637\n",
            "    ✗ MSE = 0.469203, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.265488\n",
            "      Epoch 2000/5000: Loss = 0.265887\n",
            "      Epoch 3000/5000: Loss = 0.265354\n",
            "      Epoch 4000/5000: Loss = 0.265339\n",
            "      Epoch 5000/5000: Loss = 0.265334\n",
            "    ✗ MSE = 0.273394, Time = 3.9s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.090583\n",
            "      Epoch 2000/5000: Loss = 0.090571\n",
            "      Epoch 3000/5000: Loss = 0.090570\n",
            "      Epoch 4000/5000: Loss = 0.050799\n",
            "      Epoch 5000/5000: Loss = 0.006781\n",
            "    ✓ MSE = 0.006963, Time = 4.6s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.021205\n",
            "      Epoch 2000/5000: Loss = 0.010832\n",
            "      Epoch 3000/5000: Loss = 0.002316\n",
            "      Epoch 4000/5000: Loss = 0.001554\n",
            "      Epoch 5000/5000: Loss = 0.001302\n",
            "    ✓ MSE = 0.001389, Time = 6.8s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.007944\n",
            "      Epoch 2000/5000: Loss = 0.000721\n",
            "      Epoch 3000/5000: Loss = 0.000559\n",
            "      Epoch 4000/5000: Loss = 0.000446\n",
            "      Epoch 5000/5000: Loss = 0.000646\n",
            "    ✓ MSE = 0.000661, Time = 29.2s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000409\n",
            "      Epoch 2000/5000: Loss = 0.000268\n",
            "      Epoch 3000/5000: Loss = 0.000317\n",
            "      Epoch 4000/5000: Loss = 0.000233\n",
            "      Epoch 5000/5000: Loss = 0.005580\n",
            "    ✓ MSE = 0.006871, Time = 81.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2190664010.py:526: UserWarning: Tight layout not applied. tight_layout cannot make Axes width small enough to accommodate all Axes decorations\n",
            "  plt.tight_layout(rect=[0, 0, 1, 0.98])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: results/complete/sin/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.000661\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + tanh\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.453553\n",
            "      Epoch 2000/5000: Loss = 0.451597\n",
            "      Epoch 3000/5000: Loss = 0.450915\n",
            "      Epoch 4000/5000: Loss = 0.450570\n",
            "      Epoch 5000/5000: Loss = 0.450368\n",
            "    ✗ MSE = 0.454901, Time = 3.1s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.029382\n",
            "      Epoch 2000/5000: Loss = 0.023685\n",
            "      Epoch 3000/5000: Loss = 0.010405\n",
            "      Epoch 4000/5000: Loss = 0.010279\n",
            "      Epoch 5000/5000: Loss = 0.010235\n",
            "    ✗ MSE = 0.011212, Time = 3.9s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.002125\n",
            "      Epoch 2000/5000: Loss = 0.000490\n",
            "      Epoch 3000/5000: Loss = 0.000231\n",
            "      Epoch 4000/5000: Loss = 0.000114\n",
            "      Epoch 5000/5000: Loss = 0.000072\n",
            "    ✓ MSE = 0.000075, Time = 7.5s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.001543\n",
            "      Epoch 2000/5000: Loss = 0.000148\n",
            "      Epoch 3000/5000: Loss = 0.000055\n",
            "      Epoch 4000/5000: Loss = 0.000032\n",
            "      Epoch 5000/5000: Loss = 0.000024\n",
            "    ✓ MSE = 0.000024, Time = 12.8s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.008993\n",
            "      Epoch 2000/5000: Loss = 0.001066\n",
            "      Epoch 3000/5000: Loss = 0.002765\n",
            "      Epoch 4000/5000: Loss = 0.000319\n",
            "      Epoch 5000/5000: Loss = 0.000247\n",
            "    ✓ MSE = 0.000251, Time = 60.6s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.036835\n",
            "      Epoch 2000/5000: Loss = 0.005207\n",
            "      Epoch 3000/5000: Loss = 0.001691\n",
            "      Epoch 4000/5000: Loss = 0.009061\n",
            "      Epoch 5000/5000: Loss = 0.154846\n",
            "    ✗ MSE = 0.229226, Time = 118.5s\n",
            "   Saved: results/complete/sin/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.000024\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + sigmoid\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.465054\n",
            "      Epoch 2000/5000: Loss = 0.464041\n",
            "      Epoch 3000/5000: Loss = 0.463581\n",
            "      Epoch 4000/5000: Loss = 0.463264\n",
            "      Epoch 5000/5000: Loss = 0.430126\n",
            "    ✗ MSE = 0.432922, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.037055\n",
            "      Epoch 2000/5000: Loss = 0.023923\n",
            "      Epoch 3000/5000: Loss = 0.022555\n",
            "      Epoch 4000/5000: Loss = 0.022153\n",
            "      Epoch 5000/5000: Loss = 0.022035\n",
            "    ✗ MSE = 0.022593, Time = 4.0s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.013803\n",
            "      Epoch 2000/5000: Loss = 0.000872\n",
            "      Epoch 3000/5000: Loss = 0.000264\n",
            "      Epoch 4000/5000: Loss = 0.000109\n",
            "      Epoch 5000/5000: Loss = 0.000056\n",
            "    ✓ MSE = 0.000057, Time = 5.1s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.021153\n",
            "      Epoch 2000/5000: Loss = 0.002549\n",
            "      Epoch 3000/5000: Loss = 0.000653\n",
            "      Epoch 4000/5000: Loss = 0.000282\n",
            "      Epoch 5000/5000: Loss = 0.000156\n",
            "    ✓ MSE = 0.000154, Time = 7.5s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.075192\n",
            "      Epoch 2000/5000: Loss = 0.021816\n",
            "      Epoch 3000/5000: Loss = 0.006737\n",
            "      Epoch 4000/5000: Loss = 0.003056\n",
            "      Epoch 5000/5000: Loss = 0.003695\n",
            "    ✗ MSE = 0.017041, Time = 35.7s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.313057\n",
            "      Epoch 2000/5000: Loss = 0.058071\n",
            "      Epoch 3000/5000: Loss = 0.041478\n",
            "      Epoch 4000/5000: Loss = 0.029893\n",
            "      Epoch 5000/5000: Loss = 0.021534\n",
            "    ✗ MSE = 0.022228, Time = 76.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2190664010.py:526: UserWarning: Tight layout not applied. tight_layout cannot make Axes width small enough to accommodate all Axes decorations\n",
            "  plt.tight_layout(rect=[0, 0, 1, 0.98])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Saved: results/complete/sin/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.000057\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: gaussian + relu\n",
            "======================================================================\n",
            "  Function: exp(-x²)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.096640\n",
            "      Epoch 2000/5000: Loss = 0.096640\n",
            "      Epoch 3000/5000: Loss = 0.096640\n",
            "      Epoch 4000/5000: Loss = 0.096640\n",
            "      Epoch 5000/5000: Loss = 0.096640\n",
            "    ✗ MSE = 0.095334, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000436\n",
            "      Epoch 2000/5000: Loss = 0.000208\n",
            "      Epoch 3000/5000: Loss = 0.000184\n",
            "      Epoch 4000/5000: Loss = 0.000175\n",
            "      Epoch 5000/5000: Loss = 0.000169\n",
            "    ✓ MSE = 0.000170, Time = 3.4s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000006\n",
            "      Epoch 2000/5000: Loss = 0.000003\n",
            "      Epoch 3000/5000: Loss = 0.000003\n",
            "      Epoch 4000/5000: Loss = 0.000047\n",
            "      Epoch 5000/5000: Loss = 0.000023\n",
            "    ✓ MSE = 0.000007, Time = 4.8s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000002\n",
            "      Epoch 2000/5000: Loss = 0.002735\n",
            "      Epoch 3000/5000: Loss = 0.000001\n",
            "      Epoch 4000/5000: Loss = 0.000001\n",
            "      Epoch 5000/5000: Loss = 0.000014\n",
            "    ✓ MSE = 0.000024, Time = 6.4s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000001\n",
            "      Epoch 2000/5000: Loss = 0.000001\n",
            "      Epoch 3000/5000: Loss = 0.000001\n",
            "      Epoch 4000/5000: Loss = 0.000000\n",
            "      Epoch 5000/5000: Loss = 0.000000\n",
            "    ✓ MSE = 0.000000, Time = 31.0s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000004\n",
            "      Epoch 2000/5000: Loss = 0.000001\n",
            "      Epoch 3000/5000: Loss = 0.000001\n",
            "      Epoch 4000/5000: Loss = 0.000375\n",
            "      Epoch 5000/5000: Loss = 0.000001\n",
            "    ✓ MSE = 0.000001, Time = 70.4s\n",
            "   Saved: results/complete/gaussian/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 10, Best MSE = 0.000000\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: gaussian + tanh\n",
            "======================================================================\n",
            "  Function: exp(-x²)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.095008\n",
            "      Epoch 2000/5000: Loss = 0.094653\n",
            "      Epoch 3000/5000: Loss = 0.094584\n",
            "      Epoch 4000/5000: Loss = 0.094568\n",
            "      Epoch 5000/5000: Loss = 0.094565\n",
            "    ✗ MSE = 0.093264, Time = 3.4s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000006\n",
            "      Epoch 2000/5000: Loss = 0.000004\n",
            "      Epoch 3000/5000: Loss = 0.000002\n",
            "      Epoch 4000/5000: Loss = 0.000002\n",
            "      Epoch 5000/5000: Loss = 0.000001\n",
            "    ✓ MSE = 0.000001, Time = 3.8s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000009\n",
            "      Epoch 2000/5000: Loss = 0.000004\n",
            "      Epoch 3000/5000: Loss = 0.000002\n",
            "      Epoch 4000/5000: Loss = 0.000002\n",
            "      Epoch 5000/5000: Loss = 0.000002\n",
            "    ✓ MSE = 0.000002, Time = 7.4s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000008\n",
            "      Epoch 2000/5000: Loss = 0.000011\n",
            "      Epoch 3000/5000: Loss = 0.000003\n",
            "      Epoch 4000/5000: Loss = 0.000118\n",
            "      Epoch 5000/5000: Loss = 0.000278\n",
            "    ✓ MSE = 0.000245, Time = 11.6s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000017\n",
            "      Epoch 2000/5000: Loss = 0.000011\n",
            "      Epoch 3000/5000: Loss = 0.019456\n",
            "      Epoch 4000/5000: Loss = 0.000004\n",
            "      Epoch 5000/5000: Loss = 0.015071\n",
            "    ✗ MSE = 0.022124, Time = 56.8s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000023\n",
            "      Epoch 2000/5000: Loss = 0.000010\n",
            "      Epoch 3000/5000: Loss = 0.123164\n",
            "      Epoch 4000/5000: Loss = 0.112839\n",
            "      Epoch 5000/5000: Loss = 0.000006\n",
            "    ✓ MSE = 0.000006, Time = 108.7s\n",
            "   Saved: results/complete/gaussian/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 10, Best MSE = 0.000001\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: gaussian + sigmoid\n",
            "======================================================================\n",
            "  Function: exp(-x²)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.096604\n",
            "      Epoch 2000/5000: Loss = 0.095179\n",
            "      Epoch 3000/5000: Loss = 0.094804\n",
            "      Epoch 4000/5000: Loss = 0.094659\n",
            "      Epoch 5000/5000: Loss = 0.094598\n",
            "    ✗ MSE = 0.093299, Time = 3.0s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000045\n",
            "      Epoch 2000/5000: Loss = 0.000022\n",
            "      Epoch 3000/5000: Loss = 0.000013\n",
            "      Epoch 4000/5000: Loss = 0.000007\n",
            "      Epoch 5000/5000: Loss = 0.000002\n",
            "    ✓ MSE = 0.000002, Time = 3.4s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000062\n",
            "      Epoch 2000/5000: Loss = 0.000003\n",
            "      Epoch 3000/5000: Loss = 0.000009\n",
            "      Epoch 4000/5000: Loss = 0.000002\n",
            "      Epoch 5000/5000: Loss = 0.000002\n",
            "    ✓ MSE = 0.000002, Time = 5.4s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000016\n",
            "      Epoch 2000/5000: Loss = 0.000862\n",
            "      Epoch 3000/5000: Loss = 0.000005\n",
            "      Epoch 4000/5000: Loss = 0.000007\n",
            "      Epoch 5000/5000: Loss = 0.000003\n",
            "    ✓ MSE = 0.000003, Time = 7.0s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000029\n",
            "      Epoch 2000/5000: Loss = 0.000064\n",
            "      Epoch 3000/5000: Loss = 0.000014\n",
            "      Epoch 4000/5000: Loss = 0.000012\n",
            "      Epoch 5000/5000: Loss = 0.000010\n",
            "    ✓ MSE = 0.000014, Time = 34.8s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000041\n",
            "      Epoch 2000/5000: Loss = 0.000034\n",
            "      Epoch 3000/5000: Loss = 0.000029\n",
            "      Epoch 4000/5000: Loss = 0.000025\n",
            "      Epoch 5000/5000: Loss = 0.000021\n",
            "    ✓ MSE = 0.000022, Time = 72.8s\n",
            "   Saved: results/complete/gaussian/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 10, Best MSE = 0.000002\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin_high_freq + relu\n",
            "======================================================================\n",
            "  Function: sin(2πx)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.490882\n",
            "      Epoch 2000/5000: Loss = 0.490882\n",
            "      Epoch 3000/5000: Loss = 0.490882\n",
            "      Epoch 4000/5000: Loss = 0.490882\n",
            "      Epoch 5000/5000: Loss = 0.490882\n",
            "    ✗ MSE = 0.492124, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.368177\n",
            "      Epoch 2000/5000: Loss = 0.362321\n",
            "      Epoch 3000/5000: Loss = 0.362053\n",
            "      Epoch 4000/5000: Loss = 0.362085\n",
            "      Epoch 5000/5000: Loss = 0.362010\n",
            "    ✗ MSE = 0.363075, Time = 3.4s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.269360\n",
            "      Epoch 2000/5000: Loss = 0.260874\n",
            "      Epoch 3000/5000: Loss = 0.259524\n",
            "      Epoch 4000/5000: Loss = 0.259032\n",
            "      Epoch 5000/5000: Loss = 0.258818\n",
            "    ✗ MSE = 0.259185, Time = 4.7s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.070025\n",
            "      Epoch 2000/5000: Loss = 0.049416\n",
            "      Epoch 3000/5000: Loss = 0.045417\n",
            "      Epoch 4000/5000: Loss = 0.041558\n",
            "      Epoch 5000/5000: Loss = 0.038463\n",
            "    ✗ MSE = 0.038414, Time = 5.7s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.081216\n",
            "      Epoch 2000/5000: Loss = 0.050327\n",
            "      Epoch 3000/5000: Loss = 0.045205\n",
            "      Epoch 4000/5000: Loss = 0.081896\n",
            "      Epoch 5000/5000: Loss = 0.031158\n",
            "    ✗ MSE = 0.030191, Time = 29.5s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.076661\n",
            "      Epoch 2000/5000: Loss = 0.026815\n",
            "      Epoch 3000/5000: Loss = 0.022823\n",
            "      Epoch 4000/5000: Loss = 0.020902\n",
            "      Epoch 5000/5000: Loss = 0.009640\n",
            "    ✓ MSE = 0.010106, Time = 59.8s\n",
            "   Saved: results/complete/sin_high_freq/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 1000, Best MSE = 0.010106\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin_high_freq + tanh\n",
            "======================================================================\n",
            "  Function: sin(2πx)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.489758\n",
            "      Epoch 2000/5000: Loss = 0.479542\n",
            "      Epoch 3000/5000: Loss = 0.470960\n",
            "      Epoch 4000/5000: Loss = 0.465239\n",
            "      Epoch 5000/5000: Loss = 0.462188\n",
            "    ✗ MSE = 0.465448, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.427857\n",
            "      Epoch 2000/5000: Loss = 0.203109\n",
            "      Epoch 3000/5000: Loss = 0.141505\n",
            "      Epoch 4000/5000: Loss = 0.126503\n",
            "      Epoch 5000/5000: Loss = 0.122111\n",
            "    ✗ MSE = 0.120977, Time = 3.8s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.032992\n",
            "      Epoch 2000/5000: Loss = 0.016935\n",
            "      Epoch 3000/5000: Loss = 0.010252\n",
            "      Epoch 4000/5000: Loss = 0.005540\n",
            "      Epoch 5000/5000: Loss = 0.003060\n",
            "    ✓ MSE = 0.003102, Time = 7.2s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.048397\n",
            "      Epoch 2000/5000: Loss = 0.024574\n",
            "      Epoch 3000/5000: Loss = 0.010920\n",
            "      Epoch 4000/5000: Loss = 0.005960\n",
            "      Epoch 5000/5000: Loss = 0.003559\n",
            "    ✓ MSE = 0.003691, Time = 11.0s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.421896\n",
            "      Epoch 2000/5000: Loss = 0.048623\n",
            "      Epoch 3000/5000: Loss = 0.036470\n",
            "      Epoch 4000/5000: Loss = 0.028437\n",
            "      Epoch 5000/5000: Loss = 0.026134\n",
            "    ✗ MSE = 0.026599, Time = 51.4s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.431738\n",
            "      Epoch 2000/5000: Loss = 0.245911\n",
            "      Epoch 3000/5000: Loss = 0.127498\n",
            "      Epoch 4000/5000: Loss = 0.093284\n",
            "      Epoch 5000/5000: Loss = 0.081363\n",
            "    ✗ MSE = 0.079578, Time = 105.4s\n",
            "   Saved: results/complete/sin_high_freq/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.003102\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin_high_freq + sigmoid\n",
            "======================================================================\n",
            "  Function: sin(2πx)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50, 100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.490138\n",
            "      Epoch 2000/5000: Loss = 0.489941\n",
            "      Epoch 3000/5000: Loss = 0.489816\n",
            "      Epoch 4000/5000: Loss = 0.489737\n",
            "      Epoch 5000/5000: Loss = 0.489682\n",
            "    ✗ MSE = 0.491712, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.473432\n",
            "      Epoch 2000/5000: Loss = 0.435659\n",
            "      Epoch 3000/5000: Loss = 0.295035\n",
            "      Epoch 4000/5000: Loss = 0.213814\n",
            "      Epoch 5000/5000: Loss = 0.205877\n",
            "    ✗ MSE = 0.211699, Time = 3.7s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.287873\n",
            "      Epoch 2000/5000: Loss = 0.019228\n",
            "      Epoch 3000/5000: Loss = 0.011198\n",
            "      Epoch 4000/5000: Loss = 0.008819\n",
            "      Epoch 5000/5000: Loss = 0.005103\n",
            "    ✓ MSE = 0.005282, Time = 4.9s\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.352775\n",
            "      Epoch 2000/5000: Loss = 0.055510\n",
            "      Epoch 3000/5000: Loss = 0.035578\n",
            "      Epoch 4000/5000: Loss = 0.029576\n",
            "      Epoch 5000/5000: Loss = 0.025388\n",
            "    ✗ MSE = 0.025276, Time = 7.4s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.473440\n",
            "      Epoch 2000/5000: Loss = 0.436904\n",
            "      Epoch 3000/5000: Loss = 0.151449\n",
            "      Epoch 4000/5000: Loss = 0.052781\n",
            "      Epoch 5000/5000: Loss = 0.045267\n",
            "    ✗ MSE = 0.044267, Time = 36.2s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.473514\n",
            "      Epoch 2000/5000: Loss = 0.466976\n",
            "      Epoch 3000/5000: Loss = 0.389553\n",
            "      Epoch 4000/5000: Loss = 0.334778\n",
            "      Epoch 5000/5000: Loss = 0.323169\n",
            "    ✗ MSE = 0.325167, Time = 68.6s\n",
            "   Saved: results/complete/sin_high_freq/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.005282\n",
            "Saved: full_results.csv\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE\n",
            "================================================================================\n",
            "\n",
            "Function        Activation   N_min      Best MSE     Best N    \n",
            "--------------------------------------------------------------------------------\n",
            "sin             relu         50         0.000661     500       \n",
            "sin             tanh         50         0.000024     100       \n",
            "sin             sigmoid      50         0.000057     50        \n",
            "gaussian        relu         10         0.000000     500       \n",
            "gaussian        tanh         10         0.000001     10        \n",
            "gaussian        sigmoid      10         0.000002     50        \n",
            "sin_high_freq   relu         1000       0.010106     1000      \n",
            "sin_high_freq   tanh         50         0.003102     50        \n",
            "sin_high_freq   sigmoid      50         0.005282     50        \n",
            "\n",
            "================================================================================\n",
            "ALL EXPERIMENTS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Results saved in results/ directory:\n",
            "  - results/sin/{relu,tanh,sigmoid}.png\n",
            "  - results/sin_high_freq/{relu,tanh,sigmoid}.png\n",
            "  - results/gaussian/{relu,tanh,sigmoid}.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PcR-9XRXj6"
      },
      "source": [
        "### For 1, 10, 50 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T9EuLqSb_ziE",
        "outputId": "9bd8d21a-3ba1-4ee2-f53d-ab7b3cccab46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "UNIVERSAL APPROXIMATION THEOREM: EXPERIMENTAL VALIDATION\n",
            "================================================================================\n",
            "\n",
            "This will run MxN experiments\n",
            "      (M = number of target_functions and N= number of activations)\n",
            "In our case, that is  9  experiments\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + relu\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.465637\n",
            "      Epoch 2000/5000: Loss = 0.465637\n",
            "      Epoch 3000/5000: Loss = 0.465637\n",
            "      Epoch 4000/5000: Loss = 0.465637\n",
            "      Epoch 5000/5000: Loss = 0.465637\n",
            "    ✗ MSE = 0.469203, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.265488\n",
            "      Epoch 2000/5000: Loss = 0.265887\n",
            "      Epoch 3000/5000: Loss = 0.265354\n",
            "      Epoch 4000/5000: Loss = 0.265339\n",
            "      Epoch 5000/5000: Loss = 0.265334\n",
            "    ✗ MSE = 0.273394, Time = 3.9s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.090583\n",
            "      Epoch 2000/5000: Loss = 0.090571\n",
            "      Epoch 3000/5000: Loss = 0.090570\n",
            "      Epoch 4000/5000: Loss = 0.050799\n",
            "      Epoch 5000/5000: Loss = 0.006781\n",
            "    ✓ MSE = 0.006963, Time = 4.1s\n",
            "   Saved: results/final/1-10-50/sin/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.006963\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + tanh\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.453553\n",
            "      Epoch 2000/5000: Loss = 0.451597\n",
            "      Epoch 3000/5000: Loss = 0.450915\n",
            "      Epoch 4000/5000: Loss = 0.450570\n",
            "      Epoch 5000/5000: Loss = 0.450368\n",
            "    ✗ MSE = 0.454901, Time = 3.3s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.029382\n",
            "      Epoch 2000/5000: Loss = 0.023685\n",
            "      Epoch 3000/5000: Loss = 0.010405\n",
            "      Epoch 4000/5000: Loss = 0.010279\n",
            "      Epoch 5000/5000: Loss = 0.010235\n",
            "    ✗ MSE = 0.011212, Time = 3.9s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.002125\n",
            "      Epoch 2000/5000: Loss = 0.000490\n",
            "      Epoch 3000/5000: Loss = 0.000231\n",
            "      Epoch 4000/5000: Loss = 0.000114\n",
            "      Epoch 5000/5000: Loss = 0.000072\n",
            "    ✓ MSE = 0.000075, Time = 6.6s\n",
            "   Saved: results/final/1-10-50/sin/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.000075\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + sigmoid\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.465054\n",
            "      Epoch 2000/5000: Loss = 0.464041\n",
            "      Epoch 3000/5000: Loss = 0.463581\n",
            "      Epoch 4000/5000: Loss = 0.463264\n",
            "      Epoch 5000/5000: Loss = 0.430126\n",
            "    ✗ MSE = 0.432922, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.037055\n",
            "      Epoch 2000/5000: Loss = 0.023923\n",
            "      Epoch 3000/5000: Loss = 0.022555\n",
            "      Epoch 4000/5000: Loss = 0.022153\n",
            "      Epoch 5000/5000: Loss = 0.022035\n",
            "    ✗ MSE = 0.022593, Time = 3.3s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.013803\n",
            "      Epoch 2000/5000: Loss = 0.000872\n",
            "      Epoch 3000/5000: Loss = 0.000264\n",
            "      Epoch 4000/5000: Loss = 0.000109\n",
            "      Epoch 5000/5000: Loss = 0.000056\n",
            "    ✓ MSE = 0.000057, Time = 5.5s\n",
            "   Saved: results/final/1-10-50/sin/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.000057\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: gaussian + relu\n",
            "======================================================================\n",
            "  Function: exp(-x²)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.096640\n",
            "      Epoch 2000/5000: Loss = 0.096640\n",
            "      Epoch 3000/5000: Loss = 0.096640\n",
            "      Epoch 4000/5000: Loss = 0.096640\n",
            "      Epoch 5000/5000: Loss = 0.096640\n",
            "    ✗ MSE = 0.095334, Time = 2.8s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000436\n",
            "      Epoch 2000/5000: Loss = 0.000208\n",
            "      Epoch 3000/5000: Loss = 0.000184\n",
            "      Epoch 4000/5000: Loss = 0.000175\n",
            "      Epoch 5000/5000: Loss = 0.000169\n",
            "    ✓ MSE = 0.000170, Time = 3.2s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000006\n",
            "      Epoch 2000/5000: Loss = 0.000003\n",
            "      Epoch 3000/5000: Loss = 0.000003\n",
            "      Epoch 4000/5000: Loss = 0.000047\n",
            "      Epoch 5000/5000: Loss = 0.000023\n",
            "    ✓ MSE = 0.000007, Time = 4.7s\n",
            "   Saved: results/final/1-10-50/gaussian/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 10, Best MSE = 0.000007\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: gaussian + tanh\n",
            "======================================================================\n",
            "  Function: exp(-x²)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.095008\n",
            "      Epoch 2000/5000: Loss = 0.094653\n",
            "      Epoch 3000/5000: Loss = 0.094584\n",
            "      Epoch 4000/5000: Loss = 0.094568\n",
            "      Epoch 5000/5000: Loss = 0.094565\n",
            "    ✗ MSE = 0.093264, Time = 2.8s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000006\n",
            "      Epoch 2000/5000: Loss = 0.000004\n",
            "      Epoch 3000/5000: Loss = 0.000002\n",
            "      Epoch 4000/5000: Loss = 0.000002\n",
            "      Epoch 5000/5000: Loss = 0.000001\n",
            "    ✓ MSE = 0.000001, Time = 4.0s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000009\n",
            "      Epoch 2000/5000: Loss = 0.000004\n",
            "      Epoch 3000/5000: Loss = 0.000002\n",
            "      Epoch 4000/5000: Loss = 0.000002\n",
            "      Epoch 5000/5000: Loss = 0.000002\n",
            "    ✓ MSE = 0.000002, Time = 6.9s\n",
            "   Saved: results/final/1-10-50/gaussian/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 10, Best MSE = 0.000001\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: gaussian + sigmoid\n",
            "======================================================================\n",
            "  Function: exp(-x²)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.096604\n",
            "      Epoch 2000/5000: Loss = 0.095179\n",
            "      Epoch 3000/5000: Loss = 0.094804\n",
            "      Epoch 4000/5000: Loss = 0.094659\n",
            "      Epoch 5000/5000: Loss = 0.094598\n",
            "    ✗ MSE = 0.093299, Time = 3.2s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000045\n",
            "      Epoch 2000/5000: Loss = 0.000022\n",
            "      Epoch 3000/5000: Loss = 0.000013\n",
            "      Epoch 4000/5000: Loss = 0.000007\n",
            "      Epoch 5000/5000: Loss = 0.000002\n",
            "    ✓ MSE = 0.000002, Time = 3.5s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.000062\n",
            "      Epoch 2000/5000: Loss = 0.000003\n",
            "      Epoch 3000/5000: Loss = 0.000009\n",
            "      Epoch 4000/5000: Loss = 0.000002\n",
            "      Epoch 5000/5000: Loss = 0.000002\n",
            "    ✓ MSE = 0.000002, Time = 4.7s\n",
            "   Saved: results/final/1-10-50/gaussian/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 10, Best MSE = 0.000002\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin_high_freq + relu\n",
            "======================================================================\n",
            "  Function: sin(2πx)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.490882\n",
            "      Epoch 2000/5000: Loss = 0.490882\n",
            "      Epoch 3000/5000: Loss = 0.490882\n",
            "      Epoch 4000/5000: Loss = 0.490882\n",
            "      Epoch 5000/5000: Loss = 0.490882\n",
            "    ✗ MSE = 0.492124, Time = 3.3s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.368177\n",
            "      Epoch 2000/5000: Loss = 0.362321\n",
            "      Epoch 3000/5000: Loss = 0.362053\n",
            "      Epoch 4000/5000: Loss = 0.362085\n",
            "      Epoch 5000/5000: Loss = 0.362010\n",
            "    ✗ MSE = 0.363075, Time = 3.3s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.269360\n",
            "      Epoch 2000/5000: Loss = 0.260874\n",
            "      Epoch 3000/5000: Loss = 0.259524\n",
            "      Epoch 4000/5000: Loss = 0.259032\n",
            "      Epoch 5000/5000: Loss = 0.258818\n",
            "    ✗ MSE = 0.259185, Time = 4.2s\n",
            "   Saved: results/final/1-10-50/sin_high_freq/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = None, Best MSE = 0.259185\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin_high_freq + tanh\n",
            "======================================================================\n",
            "  Function: sin(2πx)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.489758\n",
            "      Epoch 2000/5000: Loss = 0.479542\n",
            "      Epoch 3000/5000: Loss = 0.470960\n",
            "      Epoch 4000/5000: Loss = 0.465239\n",
            "      Epoch 5000/5000: Loss = 0.462188\n",
            "    ✗ MSE = 0.465448, Time = 2.9s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.427857\n",
            "      Epoch 2000/5000: Loss = 0.203109\n",
            "      Epoch 3000/5000: Loss = 0.141505\n",
            "      Epoch 4000/5000: Loss = 0.126503\n",
            "      Epoch 5000/5000: Loss = 0.122111\n",
            "    ✗ MSE = 0.120977, Time = 3.8s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.032992\n",
            "      Epoch 2000/5000: Loss = 0.016935\n",
            "      Epoch 3000/5000: Loss = 0.010252\n",
            "      Epoch 4000/5000: Loss = 0.005540\n",
            "      Epoch 5000/5000: Loss = 0.003060\n",
            "    ✓ MSE = 0.003102, Time = 7.1s\n",
            "   Saved: results/final/1-10-50/sin_high_freq/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.003102\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin_high_freq + sigmoid\n",
            "======================================================================\n",
            "  Function: sin(2πx)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [1, 10, 50]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 1 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.490138\n",
            "      Epoch 2000/5000: Loss = 0.489941\n",
            "      Epoch 3000/5000: Loss = 0.489816\n",
            "      Epoch 4000/5000: Loss = 0.489737\n",
            "      Epoch 5000/5000: Loss = 0.489682\n",
            "    ✗ MSE = 0.491712, Time = 2.8s\n",
            "\n",
            "  Training 10 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.473432\n",
            "      Epoch 2000/5000: Loss = 0.435659\n",
            "      Epoch 3000/5000: Loss = 0.295035\n",
            "      Epoch 4000/5000: Loss = 0.213814\n",
            "      Epoch 5000/5000: Loss = 0.205877\n",
            "    ✗ MSE = 0.211699, Time = 3.4s\n",
            "\n",
            "  Training 50 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.287873\n",
            "      Epoch 2000/5000: Loss = 0.019228\n",
            "      Epoch 3000/5000: Loss = 0.011198\n",
            "      Epoch 4000/5000: Loss = 0.008819\n",
            "      Epoch 5000/5000: Loss = 0.005103\n",
            "    ✓ MSE = 0.005282, Time = 5.1s\n",
            "   Saved: results/final/1-10-50/sin_high_freq/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 50, Best MSE = 0.005282\n",
            "Saved: epochs-5000.csv\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE\n",
            "================================================================================\n",
            "\n",
            "Function        Activation   N_min      Best MSE     Best N    \n",
            "--------------------------------------------------------------------------------\n",
            "sin             relu         50         0.006963     50        \n",
            "sin             tanh         50         0.000075     50        \n",
            "sin             sigmoid      50         0.000057     50        \n",
            "gaussian        relu         10         0.000007     50        \n",
            "gaussian        tanh         10         0.000001     10        \n",
            "gaussian        sigmoid      10         0.000002     50        \n",
            "sin_high_freq   relu         >5000      0.259185     50        \n",
            "sin_high_freq   tanh         50         0.003102     50        \n",
            "sin_high_freq   sigmoid      50         0.005282     50        \n",
            "\n",
            "================================================================================\n",
            "ALL EXPERIMENTS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Results saved in results/ directory:\n",
            "  - results/sin/{relu,tanh,sigmoid}.png\n",
            "  - results/sin_high_freq/{relu,tanh,sigmoid}.png\n",
            "  - results/gaussian/{relu,tanh,sigmoid}.png\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Run All Experiments\n",
        "===================\n",
        "Execute complete experimental suite for UAT validation.\n",
        "\n",
        "Usage:\n",
        "    python run_experiments.py\n",
        "\n",
        "This will generate all plots and save to results/ directory.\n",
        "\"\"\"\n",
        "\n",
        "#In case we are local, we need to import the previous code.\n",
        "#from CoreComparison import ExperimentConfig, run_single_experiment, print_comparison_table\n",
        "import itertools\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run all experiments.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"UNIVERSAL APPROXIMATION THEOREM: EXPERIMENTAL VALIDATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\\nThis will run MxN experiments\n",
        "      (M = number of target_functions and N= number of activations)\"\"\")\n",
        "\n",
        "    # Define experimental grid\n",
        "    target_functions = ['sin', 'gaussian', 'sin_high_freq']\n",
        "    activations = ['relu', 'tanh', 'sigmoid']\n",
        "    print(\"In our case, that is \",\n",
        "          len(target_functions)*len(activations),\" experiments\")\n",
        "\n",
        "    # Shared configuration\n",
        "    base_config = {\n",
        "        'neuron_counts': [1,10,50],\n",
        "        'epochs': 5000,\n",
        "        'threshold': 0.01,\n",
        "        'learning_rate': 0.01,\n",
        "        'n_samples': 3000,\n",
        "        'save_dir': 'results/final/1-10-50',\n",
        "        'verbose': True,\n",
        "        'random_seed': 42,\n",
        "    }\n",
        "\n",
        "    # Run all combinations\n",
        "    all_results = {}\n",
        "\n",
        "    for func, act in itertools.product(target_functions, activations):\n",
        "        config = ExperimentConfig(\n",
        "            target_function=func,\n",
        "            activation=act,\n",
        "            **base_config\n",
        "        )\n",
        "\n",
        "        results, metrics = run_single_experiment(config)\n",
        "        all_results[(func, act)] = (results, metrics)\n",
        "\n",
        "    # Export experiment\n",
        "    save_results(all_results,\"epochs-5000.csv\")\n",
        "    # Print comparison\n",
        "    print_comparison_table(all_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL EXPERIMENTS COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nResults saved in results/ directory:\")\n",
        "    print(\"  - results/sin/{relu,tanh,sigmoid}.png\")\n",
        "    print(\"  - results/sin_high_freq/{relu,tanh,sigmoid}.png\")\n",
        "    print(\"  - results/gaussian/{relu,tanh,sigmoid}.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm5DbnzkRb-b"
      },
      "source": [
        "### For 100, 500, 1000 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gapog4dMRt3M",
        "outputId": "14c75e7f-54ed-4550-cf31-b3cf77a0b8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "UNIVERSAL APPROXIMATION THEOREM: EXPERIMENTAL VALIDATION\n",
            "================================================================================\n",
            "\n",
            "This will run MxN experiments\n",
            "      (M = number of target_functions and N= number of activations)\n",
            "In our case, that is  3  experiments\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + relu\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.017437\n",
            "      Epoch 2000/5000: Loss = 0.003929\n",
            "      Epoch 3000/5000: Loss = 0.002650\n",
            "      Epoch 4000/5000: Loss = 0.000948\n",
            "      Epoch 5000/5000: Loss = 0.001994\n",
            "    ✓ MSE = 0.002360, Time = 6.0s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.001524\n",
            "      Epoch 2000/5000: Loss = 0.001395\n",
            "      Epoch 3000/5000: Loss = 0.005674\n",
            "      Epoch 4000/5000: Loss = 0.002983\n",
            "      Epoch 5000/5000: Loss = 0.000440\n",
            "    ✓ MSE = 0.000478, Time = 26.9s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.001171\n",
            "      Epoch 2000/5000: Loss = 0.000640\n",
            "      Epoch 3000/5000: Loss = 0.000520\n",
            "      Epoch 4000/5000: Loss = 0.000363\n",
            "      Epoch 5000/5000: Loss = 0.000283\n",
            "    ✓ MSE = 0.000278, Time = 63.1s\n",
            "   Saved: results/final/100-500-1000/sin/relu.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 100, Best MSE = 0.000278\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + tanh\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.001182\n",
            "      Epoch 2000/5000: Loss = 0.000274\n",
            "      Epoch 3000/5000: Loss = 0.000165\n",
            "      Epoch 4000/5000: Loss = 0.000120\n",
            "      Epoch 5000/5000: Loss = 0.000065\n",
            "    ✓ MSE = 0.000064, Time = 11.9s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.007540\n",
            "      Epoch 2000/5000: Loss = 0.104031\n",
            "      Epoch 3000/5000: Loss = 0.005197\n",
            "      Epoch 4000/5000: Loss = 0.000702\n",
            "      Epoch 5000/5000: Loss = 0.000354\n",
            "    ✓ MSE = 0.000341, Time = 54.9s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.032746\n",
            "      Epoch 2000/5000: Loss = 0.003436\n",
            "      Epoch 3000/5000: Loss = 0.001251\n",
            "      Epoch 4000/5000: Loss = 0.001200\n",
            "      Epoch 5000/5000: Loss = 0.000530\n",
            "    ✓ MSE = 0.000541, Time = 114.3s\n",
            "   Saved: results/final/100-500-1000/sin/tanh.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 100, Best MSE = 0.000064\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT: sin + sigmoid\n",
            "======================================================================\n",
            "  Function: sin(x)\n",
            "  Domain: [-3.0, 3.0]\n",
            "  Samples: 3000\n",
            "  Testing: [100, 500, 1000]\n",
            "  Epochs: 5000, LR: 0.01\n",
            "  Threshold: 0.01\n",
            "\n",
            "  Train Samples: 3000, Test Samples: 3000\n",
            "\n",
            "  Training 100 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.021923\n",
            "      Epoch 2000/5000: Loss = 0.005209\n",
            "      Epoch 3000/5000: Loss = 0.001533\n",
            "      Epoch 4000/5000: Loss = 0.000786\n",
            "      Epoch 5000/5000: Loss = 0.000466\n",
            "    ✓ MSE = 0.000469, Time = 7.5s\n",
            "\n",
            "  Training 500 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.077091\n",
            "      Epoch 2000/5000: Loss = 0.030077\n",
            "      Epoch 3000/5000: Loss = 0.009134\n",
            "      Epoch 4000/5000: Loss = 0.004518\n",
            "      Epoch 5000/5000: Loss = 0.002636\n",
            "    ✓ MSE = 0.002720, Time = 34.7s\n",
            "\n",
            "  Training 1000 neurons...\n",
            "      Epoch 1000/5000: Loss = 0.332920\n",
            "      Epoch 2000/5000: Loss = 0.058365\n",
            "      Epoch 3000/5000: Loss = 0.041983\n",
            "      Epoch 4000/5000: Loss = 0.032729\n",
            "      Epoch 5000/5000: Loss = 0.021911\n",
            "    ✗ MSE = 0.022631, Time = 77.5s\n",
            "   Saved: results/final/100-500-1000/sin/sigmoid.png\n",
            "\n",
            "  EXPERIMENT COMPLETE! N_min = 100, Best MSE = 0.000469\n",
            "Saved: full_results.csv\n",
            "\n",
            "================================================================================\n",
            "COMPARISON TABLE\n",
            "================================================================================\n",
            "\n",
            "Function        Activation   N_min      Best MSE     Best N    \n",
            "--------------------------------------------------------------------------------\n",
            "sin             relu         100        0.000278     1000      \n",
            "sin             tanh         100        0.000064     100       \n",
            "sin             sigmoid      100        0.000469     100       \n",
            "\n",
            "================================================================================\n",
            "ALL EXPERIMENTS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Results saved in results/ directory:\n",
            "  - results/sin/{relu,tanh,sigmoid}.png\n",
            "  - results/sin_high_freq/{relu,tanh,sigmoid}.png\n",
            "  - results/gaussian/{relu,tanh,sigmoid}.png\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Run All Experiments\n",
        "===================\n",
        "Execute complete experimental suite for UAT validation.\n",
        "\n",
        "Usage:\n",
        "    python run_experiments.py\n",
        "\n",
        "This will generate all plots and save to results/ directory.\n",
        "\"\"\"\n",
        "\n",
        "#In case we are local, we need to import the previous code.\n",
        "#from CoreComparison import ExperimentConfig, run_single_experiment, print_comparison_table\n",
        "import itertools\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run all experiments.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"UNIVERSAL APPROXIMATION THEOREM: EXPERIMENTAL VALIDATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\\nThis will run MxN experiments\n",
        "      (M = number of target_functions and N= number of activations)\"\"\")\n",
        "\n",
        "    # Define experimental grid\n",
        "    target_functions = ['sin']\n",
        "    activations = ['relu', 'tanh', 'sigmoid']\n",
        "    print(\"In our case, that is \",\n",
        "          len(target_functions)*len(activations),\" experiments\")\n",
        "\n",
        "    # Shared configuration\n",
        "    base_config = {\n",
        "        'neuron_counts': [100,500,1000],\n",
        "        'epochs': 5000,\n",
        "        'threshold': 0.01,\n",
        "        'learning_rate': 0.01,\n",
        "        'n_samples': 3000,\n",
        "        'save_dir': 'results/final/100-500-1000',\n",
        "        'verbose': True,\n",
        "        'random_seed': 42\n",
        "    }\n",
        "\n",
        "    # Run all combinations\n",
        "    all_results = {}\n",
        "\n",
        "    for func, act in itertools.product(target_functions, activations):\n",
        "        config = ExperimentConfig(\n",
        "            target_function=func,\n",
        "            activation=act,\n",
        "            **base_config\n",
        "        )\n",
        "\n",
        "        results, metrics = run_single_experiment(config)\n",
        "        all_results[(func, act)] = (results, metrics)\n",
        "\n",
        "    #Export experiment\n",
        "    save_results(all_results)\n",
        "    # Print comparison\n",
        "    print_comparison_table(all_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL EXPERIMENTS COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nResults saved in results/ directory:\")\n",
        "    print(\"  - results/sin/{relu,tanh,sigmoid}.png\")\n",
        "    print(\"  - results/sin_high_freq/{relu,tanh,sigmoid}.png\")\n",
        "    print(\"  - results/gaussian/{relu,tanh,sigmoid}.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}